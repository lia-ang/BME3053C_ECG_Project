{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lia-ang/BME3053C_ECG_Project/blob/main/final_project_ecg.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "9aBZylcwqklt"
      },
      "outputs": [],
      "source": [
        "#Group Project: Angela Liang, Ece Sarioglu, Ankitha Nath, Siri Yarrapatni, Vivian Lam, Alana Young"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kK1FuXPr3lhz"
      },
      "source": [
        "Repositories: https://github.com/huggingface/transformers/tree/main/src/transformers/models/longformer\n",
        "https://github.com/allenai/longformer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aW_cNaQS3lh1"
      },
      "source": [
        "Dataset: https://physionet.org/content/ecg-arrhythmia/1.0.0/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rbDGFKl73lh2",
        "outputId": "f5cf1479-ca30-4703-d529-134e1831bdde"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/codespace/.python/current/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7bf7df061650>"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Import libraries for data manipulation and visualization\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Import libraries for signal processing\n",
        "from scipy.signal import butter, lfilter, welch\n",
        "\n",
        "# Import libraries for machine learning and evaluation\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "\n",
        "# Import PyTorch and Hugging Face Transformers for Longformer\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import LongformerTokenizer, LongformerModel, LongformerForSequenceClassification\n",
        "\n",
        "# Import additional utilities\n",
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# For reproducibility\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "h7hHX1Ff31iR",
        "outputId": "124edb66-c06a-4da3-b5d6-dc2423d710af"
      },
      "outputs": [],
      "source": [
        "# Install required libraries\n",
        "#pip install wfdb\n",
        "#pip install transformers\n",
        "#pip install datasets\n",
        "#pip install scikit-learn\n",
        "# pip install huggingface_hub[hf_xet]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "OiQ8hKD45cS9"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import os\n",
        "import wfdb\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define the folder containing WFDB records\n",
        "folder_path = \"WFDBRecords\"\n",
        "\n",
        "# Iterate through all files in the folder\n",
        "for file in os.listdir(folder_path):\n",
        "    if file.endswith(\".hea\"):  # Process only header files\n",
        "        record_name = os.path.splitext(file)[0]  # Get the record name without extension\n",
        "        record_path = os.path.join(folder_path, record_name)\n",
        "        \n",
        "        # Load the record\n",
        "        record = wfdb.rdrecord(record_path)\n",
        "        signal = record.p_signal\n",
        "        \n",
        "        # Print the shape of the signal\n",
        "        print(f\"Processing {record_name}: Shape = {signal.shape}\")\n",
        "        \n",
        "        # Plot the first lead (e.g., Lead I)\n",
        "        plt.figure()\n",
        "        plt.plot(signal[:, 0])\n",
        "        plt.title(f\"Lead I - ECG Signal from {record_name}\")\n",
        "        plt.xlabel(\"Time (samples)\")\n",
        "        plt.ylabel(\"Amplitude (mV)\")\n",
        "        plt.grid(True)\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed batch of 100 signals. Total signals processed: 100\n",
            "Processed batch of 100 signals. Total signals processed: 100\n",
            "Processed batch of 100 signals. Total signals processed: 100\n",
            "Processed batch of 100 signals. Total signals processed: 100\n",
            "Processed batch of 100 signals. Total signals processed: 100\n",
            "Processed batch of 100 signals. Total signals processed: 100\n",
            "Processed batch of 100 signals. Total signals processed: 100\n",
            "Processed batch of 100 signals. Total signals processed: 100\n",
            "Processed batch of 100 signals. Total signals processed: 100\n",
            "Processed batch of 100 signals. Total signals processed: 100\n",
            "Processed batch of 100 signals. Total signals processed: 100\n",
            "Processed batch of 100 signals. Total signals processed: 100\n",
            "Processed batch of 100 signals. Total signals processed: 100\n",
            "Processed batch of 100 signals. Total signals processed: 100\n",
            "Processed batch of 100 signals. Total signals processed: 100\n",
            "Processed batch of 100 signals. Total signals processed: 100\n",
            "Processed batch of 100 signals. Total signals processed: 100\n",
            "Processed batch of 100 signals. Total signals processed: 100\n",
            "Processed batch of 100 signals. Total signals processed: 100\n",
            "Processed batch of 100 signals. Total signals processed: 100\n",
            "Processed batch of 100 signals. Total signals processed: 100\n",
            "Processed batch of 100 signals. Total signals processed: 100\n",
            "Processed batch of 100 signals. Total signals processed: 100\n",
            "Processed batch of 100 signals. Total signals processed: 100\n",
            "Processed batch of 100 signals. Total signals processed: 100\n",
            "Processed batch of 100 signals. Total signals processed: 100\n",
            "Processed batch of 100 signals. Total signals processed: 100\n",
            "Processed batch of 100 signals. Total signals processed: 100\n",
            "Processed batch of 100 signals. Total signals processed: 100\n",
            "Processed batch of 100 signals. Total signals processed: 100\n",
            "Processed batch of 100 signals. Total signals processed: 100\n",
            "Processed batch of 100 signals. Total signals processed: 100\n",
            "Processed batch of 100 signals. Total signals processed: 100\n",
            "Processed batch of 100 signals. Total signals processed: 100\n",
            "Processed batch of 100 signals. Total signals processed: 100\n",
            "Processed batch of 100 signals. Total signals processed: 100\n",
            "Processed batch of 100 signals. Total signals processed: 100\n",
            "Processed batch of 100 signals. Total signals processed: 100\n",
            "Processed batch of 100 signals. Total signals processed: 100\n",
            "Processed batch of 100 signals. Total signals processed: 100\n",
            "Processed batch of 100 signals. Total signals processed: 100\n",
            "Processed batch of 100 signals. Total signals processed: 100\n",
            "Processed batch of 100 signals. Total signals processed: 100\n",
            "Processed batch of 100 signals. Total signals processed: 100\n",
            "Processed batch of 100 signals. Total signals processed: 100\n",
            "Processed batch of 100 signals. Total signals processed: 100\n",
            "Processed batch of 100 signals. Total signals processed: 100\n",
            "Processed batch of 100 signals. Total signals processed: 100\n",
            "Processed batch of 100 signals. Total signals processed: 100\n",
            "Processed batch of 100 signals. Total signals processed: 100\n",
            "Processed batch of 100 signals. Total signals processed: 100\n",
            "Processed batch of 100 signals. Total signals processed: 100\n",
            "Processed batch of 100 signals. Total signals processed: 100\n",
            "Processed batch of 100 signals. Total signals processed: 100\n",
            "Processed batch of 100 signals. Total signals processed: 100\n",
            "Processed batch of 100 signals. Total signals processed: 100\n",
            "Processed batch of 100 signals. Total signals processed: 100\n",
            "Processed batch of 100 signals. Total signals processed: 100\n",
            "Processed batch of 100 signals. Total signals processed: 100\n",
            "Processed batch of 100 signals. Total signals processed: 100\n",
            "Processed batch of 100 signals. Total signals processed: 100\n",
            "Processed batch of 100 signals. Total signals processed: 100\n",
            "Processed batch of 100 signals. Total signals processed: 100\n",
            "Processed batch of 100 signals. Total signals processed: 100\n",
            "Processed batch of 100 signals. Total signals processed: 100\n",
            "Processed batch of 100 signals. Total signals processed: 100\n",
            "Processed batch of 100 signals. Total signals processed: 100\n",
            "Processed batch of 100 signals. Total signals processed: 100\n",
            "Processed batch of 100 signals. Total signals processed: 100\n",
            "Processed batch of 100 signals. Total signals processed: 100\n",
            "Processed batch of 100 signals. Total signals processed: 100\n",
            "Processed batch of 100 signals. Total signals processed: 100\n",
            "Processed batch of 100 signals. Total signals processed: 100\n",
            "Processed batch of 100 signals. Total signals processed: 100\n",
            "Processed batch of 100 signals. Total signals processed: 100\n",
            "Processed batch of 100 signals. Total signals processed: 100\n",
            "Processed batch of 100 signals. Total signals processed: 100\n",
            "Processed batch of 100 signals. Total signals processed: 100\n",
            "Processed batch of 100 signals. Total signals processed: 100\n",
            "Processed batch of 100 signals. Total signals processed: 100\n",
            "Processed batch of 100 signals. Total signals processed: 100\n",
            "Processed batch of 100 signals. Total signals processed: 100\n",
            "Processed batch of 100 signals. Total signals processed: 100\n",
            "Processed batch of 100 signals. Total signals processed: 100\n",
            "Processed batch of 100 signals. Total signals processed: 100\n",
            "Processed batch of 100 signals. Total signals processed: 100\n",
            "Processed batch of 100 signals. Total signals processed: 100\n",
            "Processed batch of 100 signals. Total signals processed: 100\n",
            "Processed batch of 100 signals. Total signals processed: 100\n",
            "Processed batch of 100 signals. Total signals processed: 100\n",
            "Processed batch of 100 signals. Total signals processed: 100\n",
            "Processed batch of 100 signals. Total signals processed: 100\n",
            "Processed batch of 100 signals. Total signals processed: 100\n",
            "Processed batch of 100 signals. Total signals processed: 100\n",
            "Processed batch of 100 signals. Total signals processed: 100\n",
            "Processed batch of 100 signals. Total signals processed: 100\n",
            "Processed batch of 100 signals. Total signals processed: 100\n",
            "Processed batch of 100 signals. Total signals processed: 100\n",
            "Processed batch of 100 signals. Total signals processed: 100\n",
            "Processed batch of 100 signals. Total signals processed: 100\n",
            "Processed batch of 100 signals. Total signals processed: 100\n",
            "Processed batch of 100 signals. Total signals processed: 100\n",
            "Processed batch of 100 signals. Total signals processed: 100\n",
            "Processed batch of 100 signals. Total signals processed: 100\n",
            "Processed batch of 100 signals. Total signals processed: 100\n",
            "Processed batch of 100 signals. Total signals processed: 100\n",
            "Processed batch of 100 signals. Total signals processed: 100\n",
            "Processed batch of 100 signals. Total signals processed: 100\n",
            "Processed batch of 100 signals. Total signals processed: 100\n",
            "Processed batch of 100 signals. Total signals processed: 100\n",
            "Processed batch of 100 signals. Total signals processed: 100\n",
            "Processed batch of 100 signals. Total signals processed: 100\n",
            "Processed batch of 100 signals. Total signals processed: 100\n",
            "Error processing JS11852: cannot reshape array of size 7998 into shape (12)\n",
            "Processed batch of 100 signals. Total signals processed: 100\n",
            "Processed final batch of 52 signals.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import wfdb\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Define the folder containing WFDB records\n",
        "folder_path = \"WFDBRecords\"\n",
        "\n",
        "# Initialize lists to store signals and labels\n",
        "signals = []\n",
        "labels = []\n",
        "\n",
        "# Recursively search for .hea files in all subdirectories\n",
        "for root, _, files in os.walk(folder_path):\n",
        "    for file in files:\n",
        "        if file.endswith(\".hea\"):  # Process only header files\n",
        "            record_name = os.path.splitext(file)[0]  # Get the record name without extension\n",
        "            record_path = os.path.join(root, record_name)\n",
        "            \n",
        "            # Load the record\n",
        "            try:\n",
        "                record = wfdb.rdrecord(record_path)\n",
        "                signal = record.p_signal\n",
        "                \n",
        "                # Append the first lead (e.g., Lead I) to the signals list\n",
        "                signals.append(signal[:, 0])  # Assuming Lead I is the first column\n",
        "                \n",
        "                # Append a placeholder label (e.g., 0 or 1) to the labels list\n",
        "                labels.append(0)  # Replace with actual label logic\n",
        "                \n",
        "                # Free memory by processing in smaller chunks\n",
        "                if len(signals) >= 100:  # Process every 100 signals\n",
        "                    signals = np.array(signals, dtype=object)\n",
        "                    labels = np.array(labels)\n",
        "                    \n",
        "                    # Save or process the batch here (e.g., write to disk or train model)\n",
        "                    print(f\"Processed batch of 100 signals. Total signals processed: {len(signals)}\")\n",
        "                    \n",
        "                    # Clear memory\n",
        "                    signals = []\n",
        "                    labels = []\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {record_name}: {e}\")\n",
        "\n",
        "#convert signals to float32 for compatibility with PyTorch\n",
        "signals = [np.array(signal, dtype=np.float32) for signal in signals]\n",
        "\n",
        "# Final processing for remaining signals\n",
        "if len(signals) > 0:\n",
        "    signals = np.array(signals, dtype=object)\n",
        "    labels = np.array(labels)\n",
        "    print(f\"Processed final batch of {len(signals)} signals.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training set size: 60\n",
            "Validation set size: 20\n",
            "Testing set size: 20\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import wfdb\n",
        "import numpy as np\n",
        "from scipy.signal import butter, lfilter\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import LongformerTokenizer\n",
        "\n",
        "# Define a bandpass filter\n",
        "def bandpass_filter(data, lowcut, highcut, fs, order=4):\n",
        "    nyquist = 0.5 * fs\n",
        "    low = lowcut / nyquist\n",
        "    high = highcut / nyquist\n",
        "    b, a = butter(order, [low, high], btype=\"band\")\n",
        "    return lfilter(b, a, data)\n",
        "\n",
        "# Sampling frequency and filter parameters\n",
        "fs = 360\n",
        "lowcut = 0.5\n",
        "highcut = 50\n",
        "\n",
        "# Define the folder containing WFDB records\n",
        "folder_path = \"WFDBRecords\"\n",
        "\n",
        "# Initialize tokenizer\n",
        "tokenizer = LongformerTokenizer.from_pretrained(\"allenai/longformer-base-4096\")\n",
        "max_length = 2048  # Reduce max length\n",
        "\n",
        "# Initialize lists to store signals and labels\n",
        "signals = []\n",
        "labels = []\n",
        "\n",
        "# Process all .hea files in the folder\n",
        "file_limit = 100  # Process only the first 100 files\n",
        "file_count = 0\n",
        "\n",
        "for root, _, files in os.walk(folder_path):\n",
        "    for file in files:\n",
        "        if file.endswith(\".hea\"):\n",
        "            file_count += 1\n",
        "            if file_count > file_limit:\n",
        "                break\n",
        "            \n",
        "            record_name = os.path.splitext(file)[0]\n",
        "            record_path = os.path.join(root, record_name)\n",
        "            try:\n",
        "                # Load the record and filter the signal\n",
        "                record = wfdb.rdrecord(record_path)\n",
        "                signal = record.p_signal[:, 0]  # Use the first lead\n",
        "                filtered_signal = bandpass_filter(signal, lowcut, highcut, fs)\n",
        "                \n",
        "                # Tokenize the signal\n",
        "                tokenized_signal = tokenizer(\n",
        "                    \" \".join(map(str, filtered_signal)),\n",
        "                    truncation=True,\n",
        "                    padding=\"max_length\",\n",
        "                    max_length=4096,\n",
        "                    return_tensors=\"pt\"\n",
        "                )\n",
        "                \n",
        "                # Append the tokenized signal and a placeholder label\n",
        "                signals.append(tokenized_signal[\"input_ids\"].squeeze(0).tolist())\n",
        "                labels.append(0)  # Replace with actual labels if available\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {record_name}: {e}\")\n",
        "\n",
        "# Convert to numpy arrays\n",
        "signals = np.array(signals, dtype=object)\n",
        "labels = np.array(labels)\n",
        "\n",
        "# Split data into training, validation, and test sets\n",
        "X_train_val, X_test, y_train_val, y_test = train_test_split(signals, labels, test_size=0.2, random_state=42)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.25, random_state=42)\n",
        "\n",
        "print(f\"Training set size: {len(X_train)}\")\n",
        "print(f\"Validation set size: {len(X_val)}\")\n",
        "print(f\"Testing set size: {len(X_test)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Custom Dataset Class\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class ECGDataset(Dataset):\n",
        "    def __init__(self, signals, labels, tokenizer, max_length=4096):\n",
        "        self.signals = signals\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.signals)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Tokenize the signal\n",
        "        tokenized = self.tokenizer(\n",
        "            \" \".join(map(str, self.signals[idx])),\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            max_length=self.max_length,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        return {\n",
        "            \"input_ids\": tokenized[\"input_ids\"].squeeze(0),\n",
        "            \"attention_mask\": tokenized[\"attention_mask\"].squeeze(0),\n",
        "            \"label\": torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# Create datasets\n",
        "# Create datasets\n",
        "train_dataset = ECGDataset(X_train, y_train, tokenizer=tokenizer)\n",
        "val_dataset = ECGDataset(X_val, y_val, tokenizer=tokenizer)\n",
        "test_dataset = ECGDataset(X_test, y_test, tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of LongformerForSequenceClassification were not initialized from the model checkpoint at allenai/longformer-base-4096 and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
            "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
            "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "# Define the Longformer Model\n",
        "\n",
        "from transformers import LongformerForSequenceClassification\n",
        "\n",
        "# Load Longformer model\n",
        "model = LongformerForSequenceClassification.from_pretrained(\"allenai/longformer-base-4096\", num_labels=2)  # Adjust num_labels for your task"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Initializing global attention on CLS token...\n"
          ]
        }
      ],
      "source": [
        "# Training the Model\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from torch.optim import AdamW\n",
        "\n",
        "# DataLoader\n",
        "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)  # Reduce batch size\n",
        "val_loader = DataLoader(val_dataset, batch_size=2)\n",
        "test_loader = DataLoader(test_dataset, batch_size=2)\n",
        "# Optimizer and loss function\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "loss_fn = CrossEntropyLoss()\n",
        "\n",
        "# Training loop with validation\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "for epoch in range(3):  # Number of epochs\n",
        "    # Training phase\n",
        "    model.train()\n",
        "    total_train_loss = 0\n",
        "    for batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"label\"].to(device)\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    \n",
        "    # Validation phase\n",
        "    model.eval()\n",
        "    total_val_loss = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch[\"label\"].to(device)\n",
        "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            loss = outputs.loss\n",
        "            total_val_loss += loss.item()\n",
        "            preds = torch.argmax(outputs.logits, dim=1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "    \n",
        "    # Print training and validation loss\n",
        "    print(f\"Epoch {epoch + 1}\")\n",
        "    print(f\"Training Loss: {total_train_loss / len(train_loader)}\")\n",
        "    print(f\"Validation Loss: {total_val_loss / len(val_loader)}\")\n",
        "    print(f\"Validation Metrics:\\n{classification_report(all_labels, all_preds)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate the Model\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "model.eval()\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"label\"].to(device)\n",
        "        outputs = model(input_ids, attention_mask=attention_mask)\n",
        "        preds = torch.argmax(outputs.logits, dim=1)\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "print(classification_report(all_labels, all_preds))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
